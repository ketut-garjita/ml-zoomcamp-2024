{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8d321a-e77b-4e17-846f-4d51a1bc0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13676610-601a-4cf0-ac92-322d8d7fb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44df9132-3c1b-423b-81d2-74b586f5f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 20:22:44.405424: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 20:22:44.416750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732800164.429807   22211 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732800164.433655   22211 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 20:22:44.446918: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a0683-683c-4616-8a03-2fb6d691cb76",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Since we have a binary classification problem, what is the best loss function for us?\n",
    "- mean squared error\n",
    "- binary crossentropy\n",
    "- categorical crossentropy\n",
    "- cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ecf2c-dc73-41fd-908e-9876942126f3",
   "metadata": {},
   "source": [
    "### Answer: binary crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e21b5-f926-40e3-97ea-00b3f6bb507a",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What's the total number of parameters of the model? You can use the summary method for that.\n",
    "- 896\n",
    "- 11214912\n",
    "- 15896912\n",
    "- 20072512\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3e012f4-f937-4a5e-b3d9-791b74303386",
   "metadata": {},
   "source": [
    "1. Input Layer\n",
    "    The input shape is (200, 200, 3).\n",
    "\n",
    "2. Conv2D Layer\n",
    "    Filters: 32\n",
    "    Kernel size: 3×33×3\n",
    "    Input channels: 3\n",
    "    Parameters per filter: 3×3×3=27\n",
    "    Total parameters for 32 filters: 27×32=864\n",
    "    Add 32 biases (one for each filter): 864+32=896\n",
    "\n",
    "3. MaxPooling2D Layer\n",
    "    Pooling size: 2×22×2\n",
    "\n",
    "4. Flatten Layer\n",
    "    Converts the output of the MaxPooling2D layer into a vector.\n",
    "    Output size of MaxPooling2D:\n",
    "    After Conv2D, the image size is 200−3+1=198 (height and width).\n",
    "    After MaxPooling2D with pool size 2×22×2: \n",
    "    198÷2=99 (height and width).\n",
    "    Final shape: 99×99×32=313632\n",
    "    Flatten turns this into a vector of size 313632.\n",
    "\n",
    "5. Dense Layer (64 neurons)\n",
    "    Input size: 313632\n",
    "    Output size: 64 neurons\n",
    "    Parameters: 313632×64=20072448\n",
    "    Add biases (64): 20072448+64=20072512\n",
    "\n",
    "6. Dense Layer (1 neuron)\n",
    "    Input size: 64\n",
    "    Output size: 1 neuron\n",
    "    Parameters: 64×1=64\n",
    "    Add bias (1): 64+1=65\n",
    "    Total Parameters\n",
    "    Conv2D: 896\n",
    "    MaxPooling2D: 0\n",
    "    Dense (64 neurons): 20072512\n",
    "    Dense (1 neuron): 65\n",
    "\n",
    "Total: 896+0+20072512+65=20072512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029220c9-7b1d-421b-ac27-047d49692d53",
   "metadata": {},
   "source": [
    "### Answer: 20072512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d1649-bdb3-4cbf-9824-e0a2f4bb75f4",
   "metadata": {},
   "source": [
    "### Generators and Training\n",
    "\n",
    "For the next two questions, use the following data generator for both train and test sets:\n",
    "\n",
    "```\n",
    "ImageDataGenerator(rescale=1./255)\n",
    "```\n",
    "\n",
    "- We don't need to do any additional pre-processing for the images.\n",
    "- When reading the data from train/test directories, check the class_mode parameter. Which value should it be for a binary classification problem?\n",
    "- Use batch_size=20\n",
    "- Use shuffle=True for both training and test sets.\n",
    "\n",
    "For training use .fit() with the following params:\n",
    "\n",
    "```\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a8522d-ecd3-41d9-9061-67b8b3433499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 201 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create the data generators\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'data/train',  \n",
    "    target_size=(200, 200),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    'data/test',  \n",
    "    target_size=(200, 200),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba21abf-4c92-4d48-91a3-59397f547b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a512639-149f-426c-bfba-160c9acc6cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 20:22:45.570157: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Create the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # For binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.002, momentum=0.8),\n",
    "    loss='binary_crossentropy',  # Binary classification loss\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2892e43-e956-4681-87fa-75ce2096d245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 352ms/step - accuracy: 0.5299 - loss: 0.7231 - val_accuracy: 0.6119 - val_loss: 0.6491\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 513ms/step - accuracy: 0.6742 - loss: 0.6105 - val_accuracy: 0.6368 - val_loss: 0.6213\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 500ms/step - accuracy: 0.6727 - loss: 0.5898 - val_accuracy: 0.5572 - val_loss: 0.6870\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 494ms/step - accuracy: 0.6791 - loss: 0.5994 - val_accuracy: 0.6468 - val_loss: 0.6272\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 515ms/step - accuracy: 0.7221 - loss: 0.5309 - val_accuracy: 0.6517 - val_loss: 0.6091\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 479ms/step - accuracy: 0.7078 - loss: 0.5535 - val_accuracy: 0.6567 - val_loss: 0.6042\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 488ms/step - accuracy: 0.7321 - loss: 0.5378 - val_accuracy: 0.6567 - val_loss: 0.6004\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 510ms/step - accuracy: 0.7714 - loss: 0.4859 - val_accuracy: 0.6219 - val_loss: 0.6643\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 512ms/step - accuracy: 0.7735 - loss: 0.4824 - val_accuracy: 0.6418 - val_loss: 0.6086\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 494ms/step - accuracy: 0.7800 - loss: 0.4757 - val_accuracy: 0.6318 - val_loss: 0.6411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f4aa22508d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec5f529-19c7-4643-ba51-7104ffe64cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 500ms/step - accuracy: 0.7956 - loss: 0.4380 - val_accuracy: 0.6866 - val_loss: 0.5892\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 549ms/step - accuracy: 0.8232 - loss: 0.4208 - val_accuracy: 0.6866 - val_loss: 0.5803\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 487ms/step - accuracy: 0.8341 - loss: 0.4053 - val_accuracy: 0.6219 - val_loss: 0.7887\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 554ms/step - accuracy: 0.7869 - loss: 0.4503 - val_accuracy: 0.6219 - val_loss: 0.7411\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 507ms/step - accuracy: 0.8536 - loss: 0.3711 - val_accuracy: 0.6816 - val_loss: 0.5840\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 506ms/step - accuracy: 0.8710 - loss: 0.3253 - val_accuracy: 0.6915 - val_loss: 0.5944\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 502ms/step - accuracy: 0.8434 - loss: 0.3455 - val_accuracy: 0.6816 - val_loss: 0.6756\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 550ms/step - accuracy: 0.8755 - loss: 0.3064 - val_accuracy: 0.6866 - val_loss: 0.5763\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 510ms/step - accuracy: 0.8928 - loss: 0.2781 - val_accuracy: 0.6866 - val_loss: 0.6039\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 507ms/step - accuracy: 0.9192 - loss: 0.2371 - val_accuracy: 0.6915 - val_loss: 0.6274\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73aa1d-9d7e-4fbb-a141-815d22f4e537",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "What is the median of training accuracy for all the epochs for this model?\n",
    "- 0.10\n",
    "- 0.32\n",
    "- 0.50\n",
    "- 0.72\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de8efb61-c7ae-4d4d-a385-d4ba0b5d3476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Accuracy: 0.8487500250339508\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract training accuracy and loss\n",
    "train_accuracy = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "\n",
    "# Calculate the median of training accuracy\n",
    "median_accuracy = np.median(train_accuracy)\n",
    "print(f\"Median Accuracy: {median_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f80822-ef7c-4b97-bb5c-2cd40fcf311c",
   "metadata": {},
   "source": [
    "### Answer: 0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2861fd-ef8a-4d12-9e40-f436bc083b13",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "What is the standard deviation of training loss for all the epochs for this model?\n",
    "- 0.028\n",
    "- 0.068\n",
    "- 0.128\n",
    "- 0.168\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6b560d6-a8a9-44a7-a3ba-547a1db0af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of Loss: 0.058114190416623\n"
     ]
    }
   ],
   "source": [
    "# Calculate the standard deviation of training loss\n",
    "std_loss = np.std(train_loss)\n",
    "print(f\"Standard Deviation of Loss: {std_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf4706-06cc-4b34-8f6f-024e3bbc14e5",
   "metadata": {},
   "source": [
    "### Answer: 0.068"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb56db-e488-4fab-8f71-de5fed942d29",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations.\n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "- rotation_range=50,\n",
    "- width_shift_range=0.1,\n",
    "- height_shift_range=0.1,\n",
    "- zoom_range=0.1,\n",
    "- horizontal_flip=True,\n",
    "- fill_mode='nearest'\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b0bbb7f-e232-4f8d-b53d-a146c1d7f862",
   "metadata": {},
   "source": [
    "# Step 1: Modify the Training Data Generator with Augmentations\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data generator with augmentations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Test generator (no augmentations applied here)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Updated generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(200, 200),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'data/test',\n",
    "    target_size=(200, 200),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50043831-d8d5-4a90-8364-b497c4aff254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 527ms/step - accuracy: 0.8952 - loss: 0.2433 - val_accuracy: 0.6915 - val_loss: 0.6463\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 517ms/step - accuracy: 0.9417 - loss: 0.1902 - val_accuracy: 0.6965 - val_loss: 0.6316\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 496ms/step - accuracy: 0.9226 - loss: 0.2132 - val_accuracy: 0.7214 - val_loss: 0.7172\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 489ms/step - accuracy: 0.9421 - loss: 0.1694 - val_accuracy: 0.7214 - val_loss: 0.6498\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 489ms/step - accuracy: 0.9207 - loss: 0.2361 - val_accuracy: 0.7065 - val_loss: 0.6386\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 424ms/step - accuracy: 0.9644 - loss: 0.1291 - val_accuracy: 0.6866 - val_loss: 0.8674\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 155ms/step - accuracy: 0.9501 - loss: 0.1400 - val_accuracy: 0.7065 - val_loss: 0.8001\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 328ms/step - accuracy: 0.9717 - loss: 0.1078 - val_accuracy: 0.7313 - val_loss: 0.7130\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - accuracy: 0.9795 - loss: 0.0942 - val_accuracy: 0.7065 - val_loss: 0.8653\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 166ms/step - accuracy: 0.9559 - loss: 0.1505 - val_accuracy: 0.7214 - val_loss: 0.7123\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Continue Training the Existing Model\n",
    "\n",
    "history_aug = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3841cfa-4d42-42e7-bf49-755996f2838d",
   "metadata": {},
   "source": [
    "## Let's train our model for 10 more epochs using the same code as previously.\n",
    "\n",
    "    Note: make sure you don't re-create the model - we want to continue training the model we already started training.\n",
    "\n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
    "- 0.26\n",
    "- 0.56\n",
    "- 0.86\n",
    "- 1.16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb11d5ac-3016-4929-bfdd-2dd589cf7afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Loss: 0.7241624534130097\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Analyze Test Loss and Accuracy\n",
    "# Test Loss (Question 5):\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Extract validation loss\n",
    "test_loss_aug = history_aug.history['val_loss']\n",
    "\n",
    "# Calculate the mean of test loss\n",
    "mean_test_loss = np.mean(test_loss_aug)\n",
    "print(f\"Mean Test Loss: {mean_test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a04cd9-7baf-4f1e-962c-37a065de331e",
   "metadata": {},
   "source": [
    "### Answer: 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b751fb3-fdde-43e0-a91c-aa769b12db67",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?\n",
    "- 0.31\n",
    "- 0.51\n",
    "- 0.71\n",
    "- 0.91\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8287c420-fa25-4897-983e-3cf7b74347d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy (last 5 epochs): 0.7104477763175965\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy (Question 6):\n",
    "\n",
    "# Extract validation accuracy\n",
    "test_accuracy_aug = history_aug.history['val_accuracy']\n",
    "\n",
    "# Calculate the average accuracy for the last 5 epochs (epochs 6 to 10)\n",
    "average_accuracy_last_5 = np.mean(test_accuracy_aug[-5:])\n",
    "print(f\"Average Test Accuracy (last 5 epochs): {average_accuracy_last_5}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f311e-c811-412a-b821-4ca4e2d987df",
   "metadata": {},
   "source": [
    "### Answer: 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48c8cf-9ec1-4520-a431-8183ac3ab941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
